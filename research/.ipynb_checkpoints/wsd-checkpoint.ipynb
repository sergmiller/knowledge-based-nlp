{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Extract definitions for all senses-candidates from WordNet (via nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_file = '../WSD_Evaluation_Framework/Data_Validation/candidatesWN30.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_candidates(file):\n",
    "    kv = defaultdict(list)\n",
    "    wn_ids = set()\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f,position=0):\n",
    "            s = line[:-1].split('\\t')\n",
    "            assert len(s) >= 3 and s[1] in ['v','a','n','r']\n",
    "            kv[(s[0],s[1])] = s[2:]\n",
    "            for t in s[2:]:\n",
    "                wn_ids.add(t)\n",
    "    return kv, wn_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155287it [00:00, 301669.99it/s]\n"
     ]
    }
   ],
   "source": [
    "w_t2wn, wn_id_set = read_candidates(candidates_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155287, 206941)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_t2wn), len(wn_id_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000000000000%1:23:01::', '1000000000000%1:23:00::']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_t2wn[('1000000000000', 'n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155287/155287 [00:15<00:00, 10309.72it/s]\n"
     ]
    }
   ],
   "source": [
    "wn2definition = defaultdict(str)\n",
    "wn2examples = defaultdict(list)\n",
    "\n",
    "for k in tqdm(w_t2wn.keys(), position=0):\n",
    "    candidates = wn.synsets(k[0])\n",
    "    assert len(candidates) > 0\n",
    "    for c in candidates:\n",
    "        for l in c.lemmas():\n",
    "            wn_id = l.key()  # something like shuffle%1:04:00::\n",
    "            if wn_id in wn_id_set:\n",
    "                wn2definition[wn_id] = c.definition()  # sentence(str)\n",
    "                if len(c.examples()) > 0:\n",
    "                    wn2examples[wn_id] = c.examples()  # list of sentences (save all of them just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206941, 59634)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wn2definition), len(wn2examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155287/155287 [00:00<00:00, 177977.10it/s]\n"
     ]
    }
   ],
   "source": [
    "w_t2wn_definition_examples = defaultdict(list)\n",
    "\n",
    "for k in tqdm(w_t2wn.keys(), position=0):\n",
    "    data = [(id, wn2definition[id], wn2examples[id]) for id in w_t2wn[k]]\n",
    "    if len(data) > 0:\n",
    "        w_t2wn_definition_examples[k] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155287"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_t2wn_definition_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king%1:18:00::', 'a male sovereign; ruler of a kingdom', []),\n",
       " ('king%1:18:02::', 'a competitor who holds a preeminent position', []),\n",
       " ('king%1:18:01::',\n",
       "  'a very wealthy or powerful businessman',\n",
       "  ['an oil baron']),\n",
       " ('king%1:26:00::',\n",
       "  'preeminence in a particular category or group or field',\n",
       "  ['the lion is the king of beasts']),\n",
       " ('king%1:18:05::', 'United States woman tennis player (born in 1943)', []),\n",
       " ('king%1:18:04::',\n",
       "  'United States guitar player and singer of the blues (born in 1925)',\n",
       "  []),\n",
       " ('king%1:18:03::',\n",
       "  'United States charismatic civil rights leader and Baptist minister who campaigned against the segregation of Blacks (1929-1968)',\n",
       "  []),\n",
       " ('king%1:06:02::',\n",
       "  \"a checker that has been moved to the opponent's first row where it is promoted to a piece that is free to move either forward or backward\",\n",
       "  []),\n",
       " ('king%1:06:01::',\n",
       "  'one of the four playing cards in a deck bearing the picture of a king',\n",
       "  []),\n",
       " ('king%1:06:00::', '(chess) the weakest but the most important piece', [])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_t2wn_definition_examples[('king', 'n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle mapping (word, tag) -> [(WN_id_1, definition_1, [example_of_definition_1_1,...]), ...(WN_id_N, definition_N, [...])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_t2wn_definition_examples = dict(w_t2wn_definition_examples)\n",
    "np.save('word_tag_2_wordnet_definition_examples.npy', w_t2wn_definition_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Prepare WSD datasets: train/eval set - SemCor, test sets - Semeval2007, Semeval2013, Semeval2015 \n",
    "(Parse xml with triplets (word, tag, sample_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_data_file = '../WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml'\n",
    "\n",
    "semval7_data_file = '../WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml'\n",
    "semval13_data_file = '../WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.data.xml'\n",
    "semval15_data_file = '../WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/semeval2015.data.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE from Semeval2007\n",
    "#\n",
    "# [('lemma', 'you'), ('pos', 'PRON')]\n",
    "# [('lemma', 'oct.'), ('pos', 'NOUN')]\n",
    "# [('lemma', '6'), ('pos', 'NUM')]\n",
    "# [('lemma', 'editorial'), ('pos', 'NOUN')]\n",
    "# [('lemma', '``'), ('pos', '.')]\n",
    "# [('lemma', 'the'), ('pos', 'DET')]\n",
    "# [('lemma', 'ill'), ('pos', 'NOUN')]\n",
    "# [('lemma', 'homeless'), ('pos', 'NOUN')]\n",
    "# [('lemma', '``'), ('pos', '.')]\n",
    "# [('id', 'd000.s000.t000'), ('lemma', 'refer'), ('pos', 'VERB')]\n",
    "# [('lemma', 'to'), ('pos', 'PRT')]\n",
    "# [('id', 'd000.s000.t001'), ('lemma', 'research'), ('pos', 'NOUN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_from_xml(file):\n",
    "\n",
    "    corpus = []\n",
    "    corpus_xml = xml.etree.ElementTree.parse(file).getroot()\n",
    "\n",
    "    for text_xml in tqdm(corpus_xml.findall('text'), position=0):\n",
    "        for sent_xml in text_xml.findall('sentence'):\n",
    "            sent = []\n",
    "            tags = []\n",
    "            target_ids = []\n",
    "            for token in sent_xml:\n",
    "                token = token.items()\n",
    "                sent.append(token[-2][1])  # \n",
    "                tags.append(str(token[-1][1][0]).lower())  # first lowercase letter of tag\n",
    "                target_ids.append(token[0][1] if len(token) > 2 else None)  #  something like d000.s000.t001 (optional)\n",
    "            corpus.append({'sentence': sent, 'tags': tags, 'target_ids': target_ids})\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 759.10it/s]\n"
     ]
    }
   ],
   "source": [
    "semeval2007 = read_corpus_from_xml(semval7_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 1133.01it/s]\n"
     ]
    }
   ],
   "source": [
    "semeval2013 = read_corpus_from_xml(semval13_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'comment', 'imply', 'we', 'have', 'discover', 'that', 'the', '``', 'principal']\n",
      "['p', 'n', 'v', 'p', 'v', 'v', 'a', 'd', '.', 'a']\n",
      "[None, 'd000.s001.t000', 'd000.s001.t001', None, None, 'd000.s001.t002', None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "print(semeval2007[1]['sentence'][:10])\n",
    "print(semeval2007[1]['tags'][:10])\n",
    "print(semeval2007[1]['target_ids'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 0. Not fine-tuned raw BERT + best cosine similarity\n",
    "\n",
    "Sentence 0 - Target lemma,\n",
    "Sentence 1 - original sentence or each possible definition (tags aren't used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance('sfdv ', list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:00<00:00, 1225.54it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = semeval2013\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenize = lambda s: tokenizer.convert_tokens_to_ids(\n",
    "    tokenizer.wordpiece_tokenizer.tokenize(' '.join(s) if not isinstance(s, str) else s))\n",
    "\n",
    "problems = []\n",
    "for sent in tqdm(corpus, position=0):\n",
    "    ids = tokenize(sent['sentence'])\n",
    "    tasks = []\n",
    "    for word, tag, target_id in zip(sent['sentence'], sent['tags'], sent['target_ids']):\n",
    "        if target_id is not None:\n",
    "            tasks.append(\n",
    "                (tokenize(word), target_id,\n",
    "                 [(sense_data[0] ,tokenize(sense_data[1])) for sense_data in w_t2wn_definition_examples[(word, tag)]]\n",
    "                ))\n",
    "    problems.append((ids, tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 229/306 [11:48<03:58,  3.09s/it]"
     ]
    }
   ],
   "source": [
    "def build_sense_embdedding(model, word, sentence):\n",
    "    tokens_tensor = torch.tensor([word + sentence])\n",
    "    segments_tensors = torch.tensor([[0] * len(word) + [1] * len(sentence)])\n",
    "\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    return torch.reshape(torch.mean(\n",
    "        encoded_layers[-1][:, :len(word), :], dim=1, keepdim=True), shape=(-1,)).detach().numpy()\n",
    "\n",
    "predicted = []\n",
    "\n",
    "for problem in tqdm(problems, position=0):\n",
    "    sentence = problem[0]\n",
    "    for task in problem[1]:\n",
    "        word, target_id, senses = task\n",
    "        labels = [s[0] for s in senses]\n",
    "        golden_sense = build_sense_embdedding(model, word, sentence)\n",
    "        definitions = np.array([build_sense_embdedding(model, word, s[1]) for s in senses])\n",
    "        dists = map(lambda s: cosine(gold_sence,s), definitions)\n",
    "        predicted.append((target_id, labels[np.argmin(dists)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predicted).to_csv('baseline_0_semval_2013', index=None, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Semeval2007 - 55.2 F1\n",
    "\n",
    "Semeval2013 - tba  F1\n",
    "\n",
    "Semeval2015 - tba  F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
